{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabajo Final Compiladores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analizador sintáctico predictivo ascendente \n",
        "Kevin Arturo Amaya Osorio"
      ],
      "metadata": {
        "id": "8_Qyk6a8j2ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Pedir la gramática LL1 y verificar la info\n"
      ],
      "metadata": {
        "id": "kKGF7kLvEx3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizador sintáctico predictivo ascendente\n",
        "      1) \n",
        "1.   Cada parte del input debe ser separado por un espacio blanco.\n",
        "2.   Las producciones de la gramática deben ser separados por '->'.\n",
        "3.   El inicio de las producciones debe empezar con S.\n",
        "4.   Una producción vacía es denotada por ~ \n",
        "\n",
        "ie: \n",
        "*   A + B -> C\n",
        "*   S ->  A B\n",
        "*   C ->  ~ "
      ],
      "metadata": {
        "id": "9wRcrhfVKeEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "wTnvUIBj4Fc4",
        "outputId": "ad5ee459-d991-4990-f8f3-58c3c3a6db52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e27bec40-115b-487d-b39b-ccc1a3293077\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e27bec40-115b-487d-b39b-ccc1a3293077\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving inputFile to inputFile (15)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "def deleteEmptyInput(input):\n",
        "  arrayInputs = [x for x in input if x != '']\n",
        "  return arrayInputs;\n",
        "\n",
        "def splitInput(inputArray): \n",
        "  inputFormatted = [deleteEmptyInput(n.split(' ')) for n in inputArray];\n",
        "  return inputFormatted;\n",
        "\n",
        "def getInputFile():\n",
        "  DATA = str(files.upload()['inputFile'])[2:-3];\n",
        "  INPUT = DATA.split('\\\\n');\n",
        "  SPLIT_ARROW_INPUT = [n.split('->') for n in INPUT];\n",
        "  PRODUCTIONS = [splitInput(n) for n in SPLIT_ARROW_INPUT];\n",
        "  return PRODUCTIONS;\n",
        "\n",
        "EMPTY_CHARACTER = '~';\n",
        "START_CHARACTER = 'S';\n",
        "PRODUCTIONS = getInputFile();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets the non terminal and terminal tokens:"
      ],
      "metadata": {
        "id": "GDBonQKSaI8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flattenList(list):\n",
        "  return [item for sublist in list for item in sublist]\n",
        "\n",
        "def getTerminalTokens(tokens, NON_TERMINAL_TOKENS):\n",
        "  return list(set(flattenList(tokens)) - set(NON_TERMINAL_TOKENS));\n",
        "\n",
        "def deleteDuplicateTokens(tokens):\n",
        "  return list(set(flattenList(tokens)))\n",
        "\n",
        "def deleteDuplicates(listDuplicates):\n",
        "  return list(set(listDuplicates));\n",
        "\n",
        "def getTerminalTokens(productions, NON_TERMINAL_TOKENS):\n",
        "  TERMINAL_TOKENS = []\n",
        "  for production in productions:\n",
        "    for token in production[1]: \n",
        "      if token not in NON_TERMINAL_TOKENS: \n",
        "        TERMINAL_TOKENS.append(token)\n",
        "  return deleteDuplicates(TERMINAL_TOKENS);\n",
        "\n",
        "def getProductionsDictionary(productions, NON_TERMINAL_TOKENS):\n",
        "  PRODUCTION_DICTIONARY = dict( [token,[]] for token in NON_TERMINAL_TOKENS);\n",
        "  for production in productions: \n",
        "    PRODUCTION_DICTIONARY[production[0][0]].append(production[1]);    \n",
        "  return PRODUCTION_DICTIONARY;\n",
        "\n",
        "NON_TERMINAL_TOKENS = deleteDuplicateTokens([n[0] for n in PRODUCTIONS])\n",
        "TERMINAL_TOKENS = getTerminalTokens(PRODUCTIONS, NON_TERMINAL_TOKENS);\n",
        "PRODUCTION_DICT = getProductionsDictionary(PRODUCTIONS, NON_TERMINAL_TOKENS);\n",
        "\n",
        "print('TERMINAL_TOKENS',TERMINAL_TOKENS)\n",
        "print('NON_TERMINAL_TOKENS',NON_TERMINAL_TOKENS)\n",
        "print('PRODUCTION_DICT',PRODUCTION_DICT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROcpQIjzVAsT",
        "outputId": "af5c4879-bb79-4e7d-89ae-81845812a161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TERMINAL_TOKENS ['~', '(', '*', 'int', '+', ')']\n",
            "NON_TERMINAL_TOKENS ['F', 'T', \"E'\", 'S', 'E', \"T'\"]\n",
            "PRODUCTION_DICT {'F': [['(', 'E', ')'], ['int']], 'T': [['F', \"T'\"]], \"E'\": [['+', 'T', \"E'\"], ['~']], 'S': [['E']], 'E': [['T', \"E'\"]], \"T'\": [['*', 'F', \"T'\"], ['~']]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting a list of first tokens by implementing the algorithm:"
      ],
      "metadata": {
        "id": "MGsdMWqjameU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isTerminal(production, TERMINAL_TOKENS):\n",
        "  return production[0] in TERMINAL_TOKENS;\n",
        "\n",
        "def first(NON_TERMINAL_TOKEN, PRODUCTION_DICT, TERMINAL_TOKENS, ORIGINAL_PROD, FIRST_RECURSION):\n",
        "  FIRST = []\n",
        "  for production in PRODUCTION_DICT[NON_TERMINAL_TOKEN]:\n",
        "    if FIRST_RECURSION: \n",
        "      ORIGINAL_PROD = production\n",
        "      \n",
        "    if( isTerminal(production, TERMINAL_TOKENS)):\n",
        "      FIRST.append({'token': production[0],'production': ORIGINAL_PROD});\n",
        "    else:\n",
        "      FIRST.extend(first(production[0], PRODUCTION_DICT, TERMINAL_TOKENS, ORIGINAL_PROD, False));\n",
        "  return FIRST;\n",
        "\n",
        "FIRSTS = dict()\n",
        "for token in NON_TERMINAL_TOKENS:\n",
        "  FIRSTS[token] = first(token, PRODUCTION_DICT, TERMINAL_TOKENS, [], True);\n",
        "\n",
        "FIRSTS_PRODUCTIONS = dict()\n",
        "for token in NON_TERMINAL_TOKENS:\n",
        "  FIRSTS_PRODUCTIONS[token] = []\n",
        "  for production in FIRSTS[token]:\n",
        "    FIRSTS_PRODUCTIONS[token].insert(0,production['token'])\n",
        "\n",
        "print('FIRSTS: ');\n",
        "print(tabulate(FIRSTS_PRODUCTIONS, headers='keys', tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdlW4I45aVf3",
        "outputId": "533ed45e-4f1e-48f5-e813-dae98d11e43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRSTS: \n",
            "╒═════╤═════╤══════╤═════╤═════╤══════╕\n",
            "│ F   │ T   │ E'   │ S   │ E   │ T'   │\n",
            "╞═════╪═════╪══════╪═════╪═════╪══════╡\n",
            "│ int │ int │ ~    │ int │ int │ ~    │\n",
            "├─────┼─────┼──────┼─────┼─────┼──────┤\n",
            "│ (   │ (   │ +    │ (   │ (   │ *    │\n",
            "╘═════╧═════╧══════╧═════╧═════╧══════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting a list of Follows tokens by implementing the algorithm:"
      ],
      "metadata": {
        "id": "HH9XprlQ4Uoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getNonRecursiveProds(token, PRODUCTIONS):\n",
        "  UNIQUE_PRODS = []\n",
        "  for production in PRODUCTIONS:\n",
        "    if token not in production:\n",
        "      UNIQUE_PRODS.append(production);\n",
        "      continue;\n",
        "    tokenIndex = production[1].index(token);\n",
        "    if tokenIndex != (len(production[1])-1):\n",
        "      UNIQUE_PRODS.append(production);\n",
        "      continue;\n",
        "  return UNIQUE_PRODS;\n",
        "\n",
        "def deleteEmptyCharacter(PRODUCTIONS):\n",
        "  if EMPTY_CHARACTER in PRODUCTIONS:\n",
        "    PRODUCTIONS.remove(EMPTY_CHARACTER);\n",
        "  return PRODUCTIONS\n",
        "\n",
        "def getProductionsToken(token, PRODUCTION_DICT):\n",
        "  PRODUCTIONS = [];\n",
        "  for key in PRODUCTION_DICT:\n",
        "    for production in PRODUCTION_DICT[key]:\n",
        "      if token in production:\n",
        "        PRODUCTIONS.append([key, production])\n",
        "  return PRODUCTIONS\n",
        "\n",
        "\n",
        "def isTerminal(token, TERMINAL_TOKENS):\n",
        "  if token in TERMINAL_TOKENS:\n",
        "    return True;\n",
        "  return False;\n",
        "\n",
        "def hasEmptyFirstProduction(token, FIRSTS_PRODUCTIONS):\n",
        "  if EMPTY_CHARACTER in FIRSTS_PRODUCTIONS[token]:\n",
        "    return True\n",
        "  return False;\n",
        "\n",
        "def follow(NON_TERMINAL_TOKEN, PRODUCTION_DICT, TERMINAL_TOKENS,FIRSTS_PRODUCTIONS):\n",
        "  FOLLOW = []\n",
        "  if(NON_TERMINAL_TOKEN == 'S'):\n",
        "      return ['$'];\n",
        "  PRODUCTIONS  = getProductionsToken(NON_TERMINAL_TOKEN, PRODUCTION_DICT);\n",
        "  UNIQUE_PRODS = getNonRecursiveProds(NON_TERMINAL_TOKEN, PRODUCTIONS);\n",
        "  try:\n",
        "    for production in UNIQUE_PRODS:\n",
        "      KEY = production[0];\n",
        "      VALUE = production[1];\n",
        "      INDEX = VALUE.index(NON_TERMINAL_TOKEN);\n",
        "      if INDEX == (len(VALUE)-1):\n",
        "        FOLLOW.extend(follow(KEY, PRODUCTION_DICT, TERMINAL_TOKENS, FIRSTS_PRODUCTIONS));\n",
        "        continue;\n",
        "      NEXT_VALUE = VALUE[INDEX + 1]\n",
        "      if(isTerminal(NEXT_VALUE, TERMINAL_TOKENS)):\n",
        "        FOLLOW.append(NEXT_VALUE);\n",
        "        continue;\n",
        "      if(hasEmptyFirstProduction(NEXT_VALUE,FIRSTS_PRODUCTIONS)):\n",
        "        FOLLOW.extend(FIRSTS_PRODUCTIONS[NEXT_VALUE]);\n",
        "        FOLLOW.extend(follow(KEY, PRODUCTION_DICT, TERMINAL_TOKENS, FIRSTS_PRODUCTIONS));\n",
        "        continue;\n",
        "      FOLLOW.extend(FIRSTS_PRODUCTIONS[NEXT_VALUE]);\n",
        "  except RecursionError as re:\n",
        "    return []\n",
        "\n",
        "  return deleteEmptyCharacter(deleteDuplicateTokens(FOLLOW));\n",
        "\n",
        "FOLLOW_PRODUCTION = dict();\n",
        "\n",
        "for token in NON_TERMINAL_TOKENS:\n",
        "  FOLLOW_PRODUCTION[token] = (follow(token, PRODUCTION_DICT, TERMINAL_TOKENS,FIRSTS_PRODUCTIONS))\n",
        "print('FOLLOW_PRODUCTION: ');\n",
        "print(tabulate(FOLLOW_PRODUCTION, headers='keys', tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJj2n9e_4LyD",
        "outputId": "0b02f141-6f4f-45ba-b177-0f90f64ca180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLLOW_PRODUCTION: \n",
            "╒═════╤═════╤══════╤═════╤═════╤══════╕\n",
            "│ F   │ T   │ E'   │ S   │ E   │ T'   │\n",
            "╞═════╪═════╪══════╪═════╪═════╪══════╡\n",
            "│ $   │ $   │ $    │ $   │ $   │ $    │\n",
            "├─────┼─────┼──────┼─────┼─────┼──────┤\n",
            "│ *   │ +   │ )    │     │ )   │ +    │\n",
            "├─────┼─────┼──────┼─────┼─────┼──────┤\n",
            "│ +   │ )   │      │     │     │ )    │\n",
            "├─────┼─────┼──────┼─────┼─────┼──────┤\n",
            "│ )   │     │      │     │     │      │\n",
            "╘═════╧═════╧══════╧═════╧═════╧══════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Parse Table"
      ],
      "metadata": {
        "id": "hNIBMgVspLdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatProduction(production,token):\n",
        "  return token +\" -> \"+\" \".join(production)\n",
        "\n",
        "def addProduction(row, indexRow, NON_TERMINAL_TOKEN, EMPTY_CHARACTER):\n",
        "  row[indexRow + 1] = NON_TERMINAL_TOKEN + ' -> ' + EMPTY_CHARACTER;\n",
        "\n",
        "def getFollowParseRow(NON_TERMINAL_TOKEN, TERMINAL_TOKENS, FOLLOW_PRODUCTION, FIRSTS, row):\n",
        "  for token in TERMINAL_TOKENS:\n",
        "    TOKEN_FOUND = False;\n",
        "    for first in FIRSTS[NON_TERMINAL_TOKEN]:\n",
        "      if token == first['token']:\n",
        "        if token == EMPTY_CHARACTER :\n",
        "          for follow in FOLLOW_PRODUCTION[NON_TERMINAL_TOKEN]:\n",
        "            indexRow = 0;\n",
        "            if follow in TERMINAL_TOKENS: \n",
        "              indexRow = TERMINAL_TOKENS.index(follow);\n",
        "            else:\n",
        "              indexRow = TERMINAL_TOKENS.index(EMPTY_CHARACTER)\n",
        "            row[indexRow + 1] = (NON_TERMINAL_TOKEN + ' -> ' + EMPTY_CHARACTER);\n",
        "        break;\n",
        "  return row;\n",
        "\n",
        "\n",
        "def getFirstParseRow(NON_TERMINAL_TOKEN, TERMINAL_TOKENS, FIRSTS, row):\n",
        "  for token in TERMINAL_TOKENS:\n",
        "    TOKEN_FOUND = False;\n",
        "    for first in FIRSTS[NON_TERMINAL_TOKEN]:\n",
        "      if token == first['token']:\n",
        "        row.append(formatProduction(first['production'],NON_TERMINAL_TOKEN))\n",
        "        TOKEN_FOUND = True;\n",
        "        break;\n",
        "    if not TOKEN_FOUND:\n",
        "        row.append('')\n",
        "  return row;\n",
        "\n",
        "def getParseRow(NON_TERMINAL_TOKEN, FIRSTS, TERMINAL_TOKENS, NON_TERMINAL_TOKENS, PRODUCTION_DICT, FOLLOW_PRODUCTION):\n",
        "  row = [NON_TERMINAL_TOKEN];\n",
        "  row = getFirstParseRow(NON_TERMINAL_TOKEN, TERMINAL_TOKENS, FIRSTS, row);\n",
        "  row = getFollowParseRow(NON_TERMINAL_TOKEN, TERMINAL_TOKENS, FOLLOW_PRODUCTION, FIRSTS, row);\n",
        "  return row\n",
        "\n",
        "parseTable = []\n",
        "\n",
        "for token in NON_TERMINAL_TOKENS:\n",
        "  parseTable.append(getParseRow(token, FIRSTS, TERMINAL_TOKENS, NON_TERMINAL_TOKENS, PRODUCTION_DICT, FOLLOW_PRODUCTION))\n",
        "\n",
        "PARSE_TABLE_HEADERS_TOKENS = [];\n",
        "PARSE_TABLE_HEADERS_TOKENS = copy.deepcopy(TERMINAL_TOKENS);\n",
        "PARSE_TABLE_HEADERS_TOKENS = list(map(lambda x: x.replace('~', '$'), PARSE_TABLE_HEADERS_TOKENS))\n",
        "PARSE_TABLE_HEADERS_TOKENS.insert(0, ' ')\n",
        "print('PARSE TABLE')\n",
        "print(tabulate(parseTable, headers=PARSE_TABLE_HEADERS_TOKENS, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H0lJ_qnixie",
        "outputId": "487db2d4-7179-4131-b7c3-ecf4422abfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PARSE TABLE\n",
            "╒═════╤═════════╤════════════╤══════════════╤═══════════╤══════════════╤═════════╕\n",
            "│     │ $       │ (          │ *            │ int       │ +            │ )       │\n",
            "╞═════╪═════════╪════════════╪══════════════╪═══════════╪══════════════╪═════════╡\n",
            "│ F   │         │ F -> ( E ) │              │ F -> int  │              │         │\n",
            "├─────┼─────────┼────────────┼──────────────┼───────────┼──────────────┼─────────┤\n",
            "│ T   │         │ T -> F T'  │              │ T -> F T' │              │         │\n",
            "├─────┼─────────┼────────────┼──────────────┼───────────┼──────────────┼─────────┤\n",
            "│ E'  │ E' -> ~ │            │              │           │ E' -> + T E' │ E' -> ~ │\n",
            "├─────┼─────────┼────────────┼──────────────┼───────────┼──────────────┼─────────┤\n",
            "│ S   │         │ S -> E     │              │ S -> E    │              │         │\n",
            "├─────┼─────────┼────────────┼──────────────┼───────────┼──────────────┼─────────┤\n",
            "│ E   │         │ E -> T E'  │              │ E -> T E' │              │         │\n",
            "├─────┼─────────┼────────────┼──────────────┼───────────┼──────────────┼─────────┤\n",
            "│ T'  │ T' -> ~ │            │ T' -> * F T' │           │ T' -> ~      │ T' -> ~ │\n",
            "╘═════╧═════════╧════════════╧══════════════╧═══════════╧══════════════╧═════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get input to generate parse tree"
      ],
      "metadata": {
        "id": "WD93e4btkMCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputWord = \"int + int\""
      ],
      "metadata": {
        "id": "58ObX-ndkLj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the output table.\n"
      ],
      "metadata": {
        "id": "mx74MlKL-fNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install treelib"
      ],
      "metadata": {
        "id": "gSgjshQuqWcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bfa440-dbe7-40a1-a262-7d06fbd42b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: treelib in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from treelib) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getRowParse(TOKEN, PARSE_TABLE):\n",
        "  for row in PARSE_TABLE:\n",
        "    if row[0] == TOKEN:\n",
        "      return row;\n",
        "  return []  \n",
        "\n",
        "def getIndexCharacter(token, TERMINAL_TOKENS):\n",
        "  if token in TERMINAL_TOKENS:\n",
        "    return TERMINAL_TOKENS.index(token)+1;\n",
        "  return -1;\n",
        "\n",
        "def offSetDerivationRow(OUTPUT_TABLE):\n",
        "    OFFSET_TABLE = copy.deepcopy(OUTPUT_TABLE);\n",
        "    for index in range(0, len(OUTPUT_TABLE)-1): \n",
        "      OFFSET_TABLE[index][2] = OFFSET_TABLE[index+1][2]\n",
        "    OFFSET_TABLE[len(OFFSET_TABLE)-1][2] = '';\n",
        "    return OFFSET_TABLE\n",
        "\n",
        "def getOutputRow(ROW, PARSE_TABLE, TERMINAL_TOKENS):\n",
        "  STACK, INPUT_WORD, DERIVATION = copy.deepcopy(ROW[0]), copy.deepcopy(ROW[1]), copy.deepcopy(ROW[2]);\n",
        "  CHARACTER_STACK = STACK[-1]\n",
        "  if (CHARACTER_STACK == INPUT_WORD[0]):\n",
        "    STACK.pop();\n",
        "    DERIVATION = '';\n",
        "    del INPUT_WORD[0]\n",
        "  else:\n",
        "    CHARACTER_STACK = STACK.pop();\n",
        "    CHARACTER_INPUT = INPUT_WORD[0];\n",
        "    INDEX_CHARACTER_INPUT = getIndexCharacter(CHARACTER_INPUT, TERMINAL_TOKENS);\n",
        "    ROW_PARSE_TABLE = getRowParse(CHARACTER_STACK, PARSE_TABLE)\n",
        "    DERIVATION = ROW_PARSE_TABLE[INDEX_CHARACTER_INPUT]\n",
        "    PRODUCTION = []\n",
        "    if(len(DERIVATION)>0):\n",
        "      PRODUCTION = deleteEmptyInput(DERIVATION.split('->')[1].split(' '));\n",
        "    STACK.extend(PRODUCTION[::-1]);     \n",
        "  return [deleteEmptyCharacter(STACK), deleteEmptyCharacter(INPUT_WORD), DERIVATION];\n",
        "\n",
        "MAX_SIZE_TABLE = 100000000000;\n",
        "OUTPUT_TABLE_HEADERS = ['Stack', 'Input', 'Derivation']\n",
        "STACK = ['$','S']\n",
        "INPUT_WORD = inputWord.split(' ');\n",
        "INPUT_WORD.append('$')\n",
        "DERIVATION = \"\"\n",
        "OUTPUT_TABLE = []\n",
        "OUTPUT_ROW = [STACK, INPUT_WORD, DERIVATION];\n",
        "OUTPUT_TABLE.append(OUTPUT_ROW)\n",
        "\n",
        "for i in range(0,MAX_SIZE_TABLE):\n",
        "  OUTPUT_ROW = getOutputRow(OUTPUT_ROW, parseTable, TERMINAL_TOKENS);\n",
        "  if OUTPUT_ROW[0] == [] and OUTPUT_ROW[1] == []:\n",
        "    break;\n",
        "  OUTPUT_TABLE.append(OUTPUT_ROW)\n",
        "\n",
        "OUTPUT_TABLE = offSetDerivationRow(OUTPUT_TABLE)\n",
        "print(tabulate(OUTPUT_TABLE, headers=OUTPUT_TABLE_HEADERS, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "id": "tJFGLQbJ-lMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5c0dde-5fc8-45f8-e66b-b55385b6bb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒══════════════════════════╤══════════════════════════╤══════════════╕\n",
            "│ Stack                    │ Input                    │ Derivation   │\n",
            "╞══════════════════════════╪══════════════════════════╪══════════════╡\n",
            "│ ['$', 'S']               │ ['int', '+', 'int', '$'] │ S -> E       │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', 'E']               │ ['int', '+', 'int', '$'] │ E -> T E'    │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", 'T']         │ ['int', '+', 'int', '$'] │ T -> F T'    │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", \"T'\", 'F']   │ ['int', '+', 'int', '$'] │ F -> int     │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", \"T'\", 'int'] │ ['int', '+', 'int', '$'] │              │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", \"T'\"]        │ ['+', 'int', '$']        │ T' -> ~      │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\"]              │ ['+', 'int', '$']        │ E' -> + T E' │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", 'T', '+']    │ ['+', 'int', '$']        │              │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", 'T']         │ ['int', '$']             │ T -> F T'    │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", \"T'\", 'F']   │ ['int', '$']             │ F -> int     │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", \"T'\", 'int'] │ ['int', '$']             │              │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\", \"T'\"]        │ ['$']                    │ T' -> ~      │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$', \"E'\"]              │ ['$']                    │ E' -> ~      │\n",
            "├──────────────────────────┼──────────────────────────┼──────────────┤\n",
            "│ ['$']                    │ ['$']                    │              │\n",
            "╘══════════════════════════╧══════════════════════════╧══════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Parse Tree"
      ],
      "metadata": {
        "id": "XSKzCM6hnDNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from treelib import Node, Tree\n",
        "\n",
        "def getRowParseTree(TOKEN, PARSE_TABLE):\n",
        "  for row in PARSE_TABLE:\n",
        "    print(TOKEN)\n",
        "    if row[0]== TOKEN['value'] :\n",
        "      return row;\n",
        "  return []  \n",
        "\n",
        "def getDerivations(OUTPUT_TABLE):\n",
        "  COLUMN = []\n",
        "  for row in OUTPUT_TABLE:\n",
        "    COLUMN.append(row[2])\n",
        "  return COLUMN;\n",
        "\n",
        "def checkEmpty(rule):\n",
        "  return rule != ''; \n",
        "\n",
        "def deleteEmptyCharacterTree(STACK):\n",
        "  TEMP_STACK = []\n",
        "  for item in STACK:\n",
        "    if item['value'] != EMPTY_CHARACTER:\n",
        "      TEMP_STACK.append(item);\n",
        "  return TEMP_STACK;\n",
        "\n",
        "def generateTree(ROW, PARSE_TABLE, TERMINAL_TOKENS, NODE_ROOT, INDEX):\n",
        "  STACK, INPUT_WORD, DERIVATION = copy.deepcopy(ROW[0]), copy.deepcopy(ROW[1]), copy.deepcopy(ROW[2]);\n",
        "  TOKEN_STACK = STACK[-1];\n",
        "  TOKEN_INPUT = INPUT_WORD[0];\n",
        "  if TOKEN_STACK['value'] == TOKEN_INPUT:\n",
        "    STACK.pop();\n",
        "    DERIVATION = '';\n",
        "    del INPUT_WORD[0]\n",
        "  #Detect that a character is a production\n",
        "  else:\n",
        "    INDEX_TOKEN_INPUT = getIndexCharacter(TOKEN_INPUT, TERMINAL_TOKENS);\n",
        "    ROW_PARSE_TABLE = getRowParse(TOKEN_STACK['value'], PARSE_TABLE);\n",
        "    DERIVATION = ROW_PARSE_TABLE[INDEX_TOKEN_INPUT].split('->');\n",
        "    DERIVATION_GENERATOR = list(filter(checkEmpty,deleteEmptyInput(DERIVATION[0])));\n",
        "    DERIVATION_PRODUCTION = list(filter(checkEmpty, DERIVATION[1].split(' ')));\n",
        "    STACK.pop();\n",
        "    PARENT_NODE = TOKEN_STACK['node'];\n",
        "    TEMP_STACK = []\n",
        "    for derProd in DERIVATION_PRODUCTION:\n",
        "      NODE_TOKEN = tree.create_node(derProd, derProd + str(INDEX), parent=PARENT_NODE.identifier);\n",
        "      TOKEN_STACK = {'value': derProd, 'node': NODE_TOKEN};\n",
        "      TEMP_STACK.append(TOKEN_STACK)\n",
        "    TEMP_STACK = deleteEmptyCharacterTree(TEMP_STACK);\n",
        "    STACK.extend(TEMP_STACK[::-1]);\n",
        "  return [STACK, INPUT_WORD, DERIVATION];\n",
        "\n",
        "\n",
        "tree = Tree()\n",
        "\n",
        "STACK = [{'value':'$', 'node':None},{'value': 'S', 'node': tree.create_node(\"S\", \"S\")}]\n",
        "INPUT_WORD = inputWord.split(' ');\n",
        "INPUT_WORD.append('$')\n",
        "DERIVATION = \"\";\n",
        "TREE_OUTPUT_ROW = [STACK, INPUT_WORD , DERIVATION];\n",
        "DERIVATIONS = deleteEmptyInput(getDerivations(OUTPUT_TABLE));\n",
        "OUTPUT_TABLE_TREE = []\n",
        "TREE_ARR = []\n",
        "OUTPUT_THREE_TABLE_HEADERS = ['Stack', 'Input', 'Derivation', 'Tree'];\n",
        "\n",
        "for j in range(0,14):\n",
        "  TREE_OUTPUT_ROW = generateTree(TREE_OUTPUT_ROW, parseTable, TERMINAL_TOKENS, None, j);\n",
        "  if TREE_OUTPUT_ROW[0] == [] and TREE_OUTPUT_ROW[1] == []:\n",
        "    break;\n",
        "  OUTPUT_TABLE_TREE.append(TREE_OUTPUT_ROW)\n",
        "\n",
        "print(tree)\n"
      ],
      "metadata": {
        "id": "Z4qC3HfVLg8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e52bebe-be89-4fbf-b23c-aa6d0a1fa0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S\n",
            "└── E\n",
            "    ├── E'\n",
            "    │   ├── +\n",
            "    │   ├── E'\n",
            "    │   │   └── ~\n",
            "    │   └── T\n",
            "    │       ├── F\n",
            "    │       │   └── int\n",
            "    │       └── T'\n",
            "    │           └── ~\n",
            "    └── T\n",
            "        ├── F\n",
            "        │   └── int\n",
            "        └── T'\n",
            "            └── ~\n",
            "\n"
          ]
        }
      ]
    }
  ]
}